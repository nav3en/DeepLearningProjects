{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs to generate Simpsons script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implenting RNNs in tensorflow to generate script for a scene from Simpsons.\n",
    "\n",
    "We are going to be using a subset of Simpsons data from the following dataset:\n",
    "https://www.kaggle.com/wcukierski/the-simpsons-by-the-data\n",
    "\n",
    "We'll be using a subset of the original dataset. It consists of only the scenes in Moe's Tavern. This doesn't include other versions of the tavern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Download and Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/simpsons/moes_tavern_lines.txt\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    input_file = os.path.join(file_path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "# Testing the load function\n",
    "data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Basic Exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11501\n",
      "Number of scenes: 263\n",
      "Average number of sentences in each scene: 15.1901140684\n",
      "Number of lines: 4258\n",
      "Average number of words in each line: 11.5044621888\n",
      "\n",
      "\n",
      "The sentences 50 to 60:\n",
      "Moe_Szyslak: Sorry, Homer.\n",
      "Homer_Simpson: You know, if you tip the glass, there won't be so much foam on top.\n",
      "Moe_Szyslak: Sorry, Homer.\n",
      "Homer_Simpson: (LOOKING AT WATCH) Ah. Finished with fifteen seconds to spare.\n",
      "Little_Man: (CONCERNED) What's the matter, buddy?\n",
      "Homer_Simpson: The moron next door closed early!\n",
      "Little_Man: (STIFFENING) I happen to be that moron.\n",
      "Homer_Simpson: Oh, me and my trenchant mouth.\n",
      "Homer_Simpson: Please, you've got to open that store.\n",
      "Little_Man: Let me think about it... Eh... No.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "view_sentence_range = (50, 60)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in data.split()})))\n",
    "scenes = data.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print('\\n')\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(data.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "\n",
    "There are a lot of preprocessing steps we can perform on text data. Some of them are:\n",
    "- Use Lookup tables (converting text to int) Helps in improving efficiency\n",
    "- Tokenize punctuation\n",
    "- Remove Stopwords\n",
    "- Remove white spaces and other charaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Lookup tables\n",
    "- vocab_to_int \n",
    "- int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(data):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # Build the vocabulary of unique words in the dataset\n",
    "    vocab = set(data)\n",
    "    \n",
    "    vocab_to_int = {word:index for index, word in enumerate(vocab)}\n",
    "    int_to_vocab = {index:word for index, word in enumerate(vocab)}\n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating lookup for punctuation\n",
    "Often it is helpful to convert punctuations to word forms \n",
    "\n",
    "Egs - . : ||Period||\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    token_dict = {\".\":\"||Period||\",\",\":\"||Comma||\",'\"':\"||Quotation_Mark||\",\";\":\"||Semicolon||\", \"!\":\"||Exclamation_Mark||\",\"?\":\"||Question_mark||\",\"(\":\"||Left_Parentheses||\",\")\":\"||Right_Parentheses||\",\"--\":\"||Dash||\", \"\\n\":\"||Return||\"}\n",
    "    \n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and store data\n",
    "Run the preprocessing code and store the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/simpsons/moes_tavern_lines.txt\"\n",
    "\n",
    "# Load the data\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "data = data[81:]\n",
    "\n",
    "# Create the lookup tables\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    data = data.replace(key, ' {} '.format(token))\n",
    "\n",
    "# Convert to lower case and split the data\n",
    "data = data.lower().split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(data)\n",
    "int_text = [vocab_to_int[word] for word in data]\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Test if you can load the stored data\n",
    "\n",
    "data = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Network\n",
    "\n",
    "- Define placeholders\n",
    "- Create word embeddings\n",
    "- Create the RNN cells with dropout and initialize the state\n",
    "- Create the RNN network\n",
    "- Create the fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create the input, targets and learning rate placeholders\n",
    "def get_inputs():\n",
    "    inputs = tf.placeholder(dtype=tf.int32,shape = [None, None], name=\"inputs\")\n",
    "    targets  = tf.placeholder(tf.int32, [None, None], name =\"targets\")\n",
    "    learning_rate = tf.placeholder(tf.float32,name =\"learning_rate\")\n",
    "    \n",
    "    return (inputs, targets, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and stack the RNNs cells and initialize them\n",
    "Stack single or multiple RNN cells with dropout and initialize the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_cells(rnn_size, batch_size, num_layers = 1, keep_prob=0.7):\n",
    "    \n",
    "    ## Define cell, dropout and multicell\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropout]*num_layers)\n",
    "    \n",
    "    ## Initialize the RNN cell\n",
    "    initial_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "    ## Using tf.identity to set the name of the initial state                    \n",
    "    initial_state = tf.identity(initial_state,name = \"initial_state\")\n",
    "    \n",
    "    return(cell, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the embedding layer\n",
    "Since there are a huge number of words, instead of passing in a one hot encoded vector as input, \n",
    "it would be more efficient to use word embeddings. \n",
    "Since this is a small network, we can train the embeddings as a part of the same network\n",
    "Usually in larger networks and when inputs are larger it is way more efficient to generate the embeddings separately \n",
    "and then feed it to this network. This would avoid the embeddings being trained every time we train this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_layer(inputs, vocab_size, embedding_size):\n",
    "    embedding_weights = tf.Variable(tf.random_uniform((vocab_size, embedding_size),-1,1))\n",
    "    embedding = tf.nn.embedding_lookup(embedding_weights,inputs)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Building the RNN\n",
    "\n",
    "def build_rnn(cell, inputs):\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell,inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state,name = \"final_state\")\n",
    "    return (outputs,final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Building the completing network\n",
    "\n",
    "def build_network(cell, input_data, vocab_size, rnn_size):\n",
    "    # Get the embeddings, Here we are using the rnn_size as the embedding size, but we don't have to\n",
    "    embed = get_embedding_layer(input_data, vocab_size, rnn_size)\n",
    "    # Build the RNN passing the embeddings\n",
    "    rnn_outputs, final_state = build_rnn(cell, embed)\n",
    "    # Build a fully connected layer  with hthe weights and biases initialized\n",
    "    logits = tf.contrib.layers.fully_connected(rnn_outputs, num_outputs = vocab_size, weights_initializer=tf.truncated_normal_initializer(stddev=0.1),biases_initializer=tf.zeros_initializer())\n",
    "    return (logits,final_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Batches\n",
    "We are going to generate the batches which are going to be fed into the network\n",
    "\n",
    "The batches should be a Numpy array with the shape (number of batches, 2, batch size, sequence length). Each batch contains two elements:\n",
    "\n",
    "The first element is a single batch of input with the shape [batch size, sequence length]\n",
    "The second element is a single batch of targets with the shape [batch size, sequence length]\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "For exmple, get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3) would return a Numpy array of the following:\n",
    "[\n",
    "  #### First Batch\n",
    "  [\n",
    "    ##### Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    ##### Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    "\n",
    "  #### Second Batch\n",
    "  [\n",
    "    ##### Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    ##### Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    int_text = np.array(int_text)\n",
    "      \n",
    "    split_size = batch_size * seq_length\n",
    "    n_batches = int(len(int_text) / split_size)\n",
    "\n",
    "\n",
    "    x = int_text[: n_batches*split_size]\n",
    "    y = int_text[1: n_batches*split_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    xx = [x[:, i*seq_length:i*seq_length+seq_length].tolist() for i in range(n_batches)]\n",
    "    yy = [y[:, i*seq_length:i*seq_length+seq_length].tolist() for i in range(n_batches)]\n",
    " \n",
    "    batches = np.array([a for a in zip(xx,yy)])\n",
    "    return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 128\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "learning_rate_value = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "## Save directory\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, learning_rate = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_rnn_cells(rnn_size, input_data_shape[0], num_layers = 1, keep_prob=0.7)\n",
    "    logits, final_state = build_network(cell, input_text, vocab_size, rnn_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/26   train_loss = 8.873\n",
      "Epoch   0 Batch   10/26   train_loss = 6.069\n",
      "Epoch   0 Batch   20/26   train_loss = 5.781\n",
      "Epoch   1 Batch    4/26   train_loss = 5.447\n",
      "Epoch   1 Batch   14/26   train_loss = 5.275\n",
      "Epoch   1 Batch   24/26   train_loss = 5.335\n",
      "Epoch   2 Batch    8/26   train_loss = 5.097\n",
      "Epoch   2 Batch   18/26   train_loss = 5.052\n",
      "Epoch   3 Batch    2/26   train_loss = 4.815\n",
      "Epoch   3 Batch   12/26   train_loss = 5.017\n",
      "Epoch   3 Batch   22/26   train_loss = 4.788\n",
      "Epoch   4 Batch    6/26   train_loss = 4.809\n",
      "Epoch   4 Batch   16/26   train_loss = 4.717\n",
      "Epoch   5 Batch    0/26   train_loss = 4.529\n",
      "Epoch   5 Batch   10/26   train_loss = 4.594\n",
      "Epoch   5 Batch   20/26   train_loss = 4.637\n",
      "Epoch   6 Batch    4/26   train_loss = 4.549\n",
      "Epoch   6 Batch   14/26   train_loss = 4.473\n",
      "Epoch   6 Batch   24/26   train_loss = 4.531\n",
      "Epoch   7 Batch    8/26   train_loss = 4.385\n",
      "Epoch   7 Batch   18/26   train_loss = 4.484\n",
      "Epoch   8 Batch    2/26   train_loss = 4.167\n",
      "Epoch   8 Batch   12/26   train_loss = 4.511\n",
      "Epoch   8 Batch   22/26   train_loss = 4.297\n",
      "Epoch   9 Batch    6/26   train_loss = 4.323\n",
      "Epoch   9 Batch   16/26   train_loss = 4.326\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Get the batches\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "# Initialize a session with the graph\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                learning_rate: learning_rate_value}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Save params\n",
    "params = (seq_length, save_dir)\n",
    "pickle.dump(params, open('params.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the new script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the preprocessed data and model params\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Tensors helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"inputs:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    \n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement pick word function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Pick most probable word\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    probabilities = probabilities.tolist()\n",
    "    predicted_word = int_to_vocab[probabilities.index(max(probabilities))]\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Pick Random Word\n",
    "def pick_random_word(probabilities, int_to_vocab):\n",
    "\n",
    "    t = np.cumsum(probabilities)\n",
    "    rand_s = np.sum(probabilities) * np.random.rand(1)\n",
    "    pred_word = int_to_vocab[int(np.searchsorted(t, rand_s))]\n",
    "\n",
    "    return pred_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the New Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak:(sings) yeah, but i got a woman.\n",
      "moe_szyslak:(to moe) oh, it's probably a man enough left a man in.\n",
      "homer_simpson:(singing) i am.\n",
      "moe_szyslak:(sobs)\n",
      "homer_simpson:(to homer) i want to that.\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(terrified noise)\n",
      "homer_simpson:(\") moe, i got a beer.\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(warily) oh, i can't believe it has a big company.\n",
      "homer_simpson:(loud) hey, what's the springfield, please!\n",
      "moe_szyslak:(laughs)\n",
      "homer_simpson:(to homer, homer) i got a\" love...\n",
      "homer_simpson:(singing)\"(\"\")\"(\"\"\" and\".\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(to camera) you know, i see...\n",
      "homer_simpson:(warily) oh, that's it.\n",
      "homer_simpson:(reading)\" and, and you sure, and you all the end of my friend.\n",
      "moe_szyslak:(sobs)\n",
      "homer_simpson:(sobs)\n",
      "homer_simpson:(to moe) hey, what's the world of the bar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 250\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on Improving the Model\n",
    "\n",
    "Tweaking the hyperparameters will lead to better output\n",
    "Training on more data is a good way to improve the model\n",
    "Adding more preprocessing to the inputs might help in improving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF Python 3",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
